**å›¾åƒåˆ†ç±»**æ˜¯æ ¹æ®å›¾åƒçš„è¯­ä¹‰ä¿¡æ¯å¯¹ä¸åŒç±»åˆ«å›¾åƒè¿›è¡ŒåŒºåˆ†ï¼Œæ˜¯è®¡ç®—æœºè§†è§‰çš„æ ¸å¿ƒï¼Œæ˜¯ç‰©ä½“æ£€æµ‹ã€å›¾åƒåˆ†å‰²ã€ç‰©ä½“è·Ÿè¸ªã€è¡Œä¸ºåˆ†æã€äººè„¸è¯†åˆ«ç­‰å…¶ä»–é«˜å±‚æ¬¡è§†è§‰ä»»åŠ¡çš„åŸºç¡€ã€‚

åŸºäºçœ¼ç–¾åˆ†ç±»æ•°æ®é›†iChallenge-PMï¼Œå¯¹å›¾åƒåˆ†ç±»é¢†åŸŸçš„ç»å…¸å·ç§¯ç¥ç»ç½‘ç»œè¿›è¡Œå‰–æï¼Œä»‹ç»å¦‚ä½•åº”ç”¨è¿™äº›åŸºç¡€æ¨¡å—æ„å»ºå·ç§¯ç¥ç»ç½‘ç»œï¼Œè§£å†³å›¾åƒåˆ†ç±»é—®é¢˜ã€‚

# LeNet

LeNeté€šè¿‡è¿ç»­ä½¿ç”¨å·ç§¯å’Œæ± åŒ–å±‚çš„ç»„åˆæå–å›¾åƒç‰¹å¾

![1714442767247](image/02å›¾åƒåˆ†ç±»/1714442767247.png)

* ç¬¬ä¸€æ¨¡å—ï¼šåŒ…å«5Ã—5çš„6é€šé“å·ç§¯å’Œ2Ã—2çš„æ± åŒ–ã€‚å·ç§¯æå–å›¾åƒä¸­åŒ…å«çš„ç‰¹å¾æ¨¡å¼ï¼ˆæ¿€æ´»å‡½æ•°ä½¿ç”¨Sigmoidï¼‰ï¼Œå›¾åƒå°ºå¯¸ä»28å‡å°åˆ°24ã€‚ç»è¿‡æ± åŒ–å±‚å¯ä»¥é™ä½è¾“å‡ºç‰¹å¾å›¾å¯¹ç©ºé—´ä½ç½®çš„æ•æ„Ÿæ€§ï¼Œå›¾åƒå°ºå¯¸å‡åˆ°12ã€‚
* ç¬¬äºŒæ¨¡å—ï¼šå’Œç¬¬ä¸€æ¨¡å—å°ºå¯¸ç›¸åŒï¼Œé€šé“æ•°ç”±6å¢åŠ ä¸º16ã€‚å·ç§¯æ“ä½œä½¿å›¾åƒå°ºå¯¸å‡å°åˆ°8ï¼Œç»è¿‡æ± åŒ–åå˜æˆ4ã€‚
* ç¬¬ä¸‰æ¨¡å—ï¼šåŒ…å«4Ã—4çš„120é€šé“å·ç§¯ã€‚å·ç§¯ä¹‹åçš„å›¾åƒå°ºå¯¸å‡å°åˆ°1ï¼Œä½†æ˜¯é€šé“æ•°å¢åŠ ä¸º120ã€‚å°†ç»è¿‡ç¬¬3æ¬¡å·ç§¯æå–åˆ°çš„ç‰¹å¾å›¾è¾“å…¥åˆ°å…¨è¿æ¥å±‚ã€‚ç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚çš„è¾“å‡ºç¥ç»å…ƒçš„ä¸ªæ•°æ˜¯64ï¼Œç¬¬äºŒä¸ªå…¨è¿æ¥å±‚çš„è¾“å‡ºç¥ç»å…ƒä¸ªæ•°æ˜¯åˆ†ç±»æ ‡ç­¾çš„ç±»åˆ«æ•°ï¼Œå¯¹äºæ‰‹å†™æ•°å­—è¯†åˆ«çš„ç±»åˆ«æ•°æ˜¯10ã€‚ç„¶åä½¿ç”¨Softmaxæ¿€æ´»å‡½æ•°å³å¯è®¡ç®—å‡ºæ¯ä¸ªç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡ã€‚

## LeNetåœ¨æ‰‹å†™æ•°å­—è¯†åˆ«ä¸Šçš„åº”ç”¨

```
# å¯¼å…¥éœ€è¦çš„åŒ…
import paddle
import numpy as np
from paddle.nn import Conv2D, MaxPool2D, Linear

## ç»„ç½‘
import paddle.nn.functional as F

# å®šä¹‰ LeNet ç½‘ç»œç»“æ„
class LeNet(paddle.nn.Layer):
    def __init__(self, num_classes=1):
        super(LeNet, self).__init__()
        # åˆ›å»ºå·ç§¯å’Œæ± åŒ–å±‚
        # åˆ›å»ºç¬¬1ä¸ªå·ç§¯å±‚
        self.conv1 = Conv2D(in_channels=1, out_channels=6, kernel_size=5)
        self.max_pool1 = MaxPool2D(kernel_size=2, stride=2)
        # å°ºå¯¸çš„é€»è¾‘ï¼šæ± åŒ–å±‚æœªæ”¹å˜é€šé“æ•°ï¼›å½“å‰é€šé“æ•°ä¸º6
        # åˆ›å»ºç¬¬2ä¸ªå·ç§¯å±‚
        self.conv2 = Conv2D(in_channels=6, out_channels=16, kernel_size=5)
        self.max_pool2 = MaxPool2D(kernel_size=2, stride=2)
        # åˆ›å»ºç¬¬3ä¸ªå·ç§¯å±‚
        self.conv3 = Conv2D(in_channels=16, out_channels=120, kernel_size=4)
        # å°ºå¯¸çš„é€»è¾‘ï¼šè¾“å…¥å±‚å°†æ•°æ®æ‹‰å¹³[B,C,H,W] -> [B,C*H*W]
        # è¾“å…¥sizeæ˜¯[28,28]ï¼Œç»è¿‡ä¸‰æ¬¡å·ç§¯å’Œä¸¤æ¬¡æ± åŒ–ä¹‹åï¼ŒC*H*Wç­‰äº120
        self.fc1 = Linear(in_features=120, out_features=64)
        # åˆ›å»ºå…¨è¿æ¥å±‚ï¼Œç¬¬ä¸€ä¸ªå…¨è¿æ¥å±‚çš„è¾“å‡ºç¥ç»å…ƒä¸ªæ•°ä¸º64ï¼Œ ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚è¾“å‡ºç¥ç»å…ƒä¸ªæ•°ä¸ºåˆ†ç±»æ ‡ç­¾çš„ç±»åˆ«æ•°
        self.fc2 = Linear(in_features=64, out_features=num_classes)
    # ç½‘ç»œçš„å‰å‘è®¡ç®—è¿‡ç¨‹
    def forward(self, x):
        x = self.conv1(x)
        # æ¯ä¸ªå·ç§¯å±‚ä½¿ç”¨Sigmoidæ¿€æ´»å‡½æ•°ï¼Œåé¢è·Ÿç€ä¸€ä¸ª2x2çš„æ± åŒ–
        x = F.sigmoid(x)
        x = self.max_pool1(x)
        x = F.sigmoid(x)
        x = self.conv2(x)
        x = self.max_pool2(x)
        x = self.conv3(x)
        # å°ºå¯¸çš„é€»è¾‘ï¼šè¾“å…¥å±‚å°†æ•°æ®æ‹‰å¹³[B,C,H,W] -> [B,C*H*W]
        x = paddle.reshape(x, [x.shape[0], -1])
        x = self.fc1(x)
        x = F.sigmoid(x)
        x = self.fc2(x)
        return x
```

# AlexNet

AlexNetä¸LeNetç›¸æ¯”ï¼Œå…·æœ‰æ›´æ·±çš„ç½‘ç»œç»“æ„ï¼ŒåŒ…å«5å±‚å·ç§¯å’Œ3å±‚å…¨è¿æ¥ï¼ŒåŒæ—¶ä½¿ç”¨äº†å¦‚ä¸‹ä¸‰ç§æ–¹æ³•æ”¹è¿›æ¨¡å‹çš„è®­ç»ƒè¿‡ç¨‹ï¼š

* æ•°æ®å¢å¹¿ï¼šæ·±åº¦å­¦ä¹ ä¸­å¸¸ç”¨çš„ä¸€ç§å¤„ç†æ–¹å¼ï¼Œé€šè¿‡å¯¹è®­ç»ƒéšæœºåŠ ä¸€äº›å˜åŒ–ï¼Œæ¯”å¦‚å¹³ç§»ã€ç¼©æ”¾ã€è£å‰ªã€æ—‹è½¬ã€ç¿»è½¬æˆ–è€…å¢å‡äº®åº¦ç­‰ï¼Œäº§ç”Ÿä¸€ç³»åˆ—è·ŸåŸå§‹å›¾ç‰‡ç›¸ä¼¼ä½†åˆä¸å®Œå…¨ç›¸åŒçš„æ ·æœ¬ï¼Œä»è€Œæ‰©å¤§è®­ç»ƒæ•°æ®é›†ã€‚é€šè¿‡è¿™ç§æ–¹å¼ï¼Œå¯ä»¥éšæœºæ”¹å˜è®­ç»ƒæ ·æœ¬ï¼Œé¿å…æ¨¡å‹è¿‡åº¦ä¾èµ–äºæŸäº›å±æ€§ï¼Œèƒ½ä»ä¸€å®šç¨‹åº¦ä¸ŠæŠ‘åˆ¶è¿‡æ‹Ÿåˆã€‚
* ä½¿ç”¨DropoutæŠ‘åˆ¶è¿‡æ‹Ÿåˆã€‚
* ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°å‡å°‘æ¢¯åº¦æ¶ˆå¤±ç°è±¡ã€‚

![1715001411798](image/02å›¾åƒåˆ†ç±»/1715001411798.png)

```
# -*- coding:utf-8 -*-

# å¯¼å…¥éœ€è¦çš„åŒ…
import paddle
import numpy as np
from paddle.nn import Conv2D, MaxPool2D, Linear, Dropout
## ç»„ç½‘
import paddle.nn.functional as F

# å®šä¹‰ AlexNet ç½‘ç»œç»“æ„
class AlexNet(paddle.nn.Layer):
    def __init__(self, num_classes=1):
        super(AlexNet, self).__init__()
        # AlexNetä¸LeNetä¸€æ ·ä¹Ÿä¼šåŒæ—¶ä½¿ç”¨å·ç§¯å’Œæ± åŒ–å±‚æå–å›¾åƒç‰¹å¾
        # ä¸LeNetä¸åŒçš„æ˜¯æ¿€æ´»å‡½æ•°æ¢æˆäº†â€˜reluâ€™
        self.conv1 = Conv2D(in_channels=3, out_channels=96, kernel_size=11, stride=4, padding=5)
        self.max_pool1 = MaxPool2D(kernel_size=2, stride=2)
        self.conv2 = Conv2D(in_channels=96, out_channels=256, kernel_size=5, stride=1, padding=2)
        self.max_pool2 = MaxPool2D(kernel_size=2, stride=2)
        self.conv3 = Conv2D(in_channels=256, out_channels=384, kernel_size=3, stride=1, padding=1)
        self.conv4 = Conv2D(in_channels=384, out_channels=384, kernel_size=3, stride=1, padding=1)
        self.conv5 = Conv2D(in_channels=384, out_channels=256, kernel_size=3, stride=1, padding=1)
        self.max_pool5 = MaxPool2D(kernel_size=2, stride=2)

        self.fc1 = Linear(in_features=12544, out_features=4096)
        self.drop_ratio1 = 0.5
        self.drop1 = Dropout(self.drop_ratio1)
        self.fc2 = Linear(in_features=4096, out_features=4096)
        self.drop_ratio2 = 0.5
        self.drop2 = Dropout(self.drop_ratio2)
        self.fc3 = Linear(in_features=4096, out_features=num_classes)
  
    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.max_pool1(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = self.max_pool2(x)
        x = self.conv3(x)
        x = F.relu(x)
        x = self.conv4(x)
        x = F.relu(x)
        x = self.conv5(x)
        x = F.relu(x)
        x = self.max_pool5(x)
        x = paddle.reshape(x, [x.shape[0], -1])
        x = self.fc1(x)
        x = F.relu(x)
        # åœ¨å…¨è¿æ¥ä¹‹åä½¿ç”¨dropoutæŠ‘åˆ¶è¿‡æ‹Ÿåˆ
        x = self.drop1(x)
        x = self.fc2(x)
        x = F.relu(x)
        # åœ¨å…¨è¿æ¥ä¹‹åä½¿ç”¨dropoutæŠ‘åˆ¶è¿‡æ‹Ÿåˆ
        x = self.drop2(x)
        x = self.fc3(x)
        return x
```

```
# åˆ›å»ºæ¨¡å‹
model = AlexNet()
# å¯åŠ¨è®­ç»ƒè¿‡ç¨‹
opt = paddle.optimizer.Adam(learning_rate=0.001, parameters=model.parameters())

train_pm(model, optimizer=opt)
```

é€šè¿‡è¿è¡Œç»“æœå¯ä»¥å‘ç°ï¼Œåœ¨çœ¼ç–¾ç­›æŸ¥æ•°æ®é›†iChallenge-PMä¸Šä½¿ç”¨AlexNetï¼Œlossèƒ½æœ‰æ•ˆä¸‹é™ï¼Œç»è¿‡5ä¸ªepochçš„è®­ç»ƒï¼Œåœ¨éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡å¯ä»¥è¾¾åˆ°94%å·¦å³ã€‚

# VGG

VGGæ˜¯å½“å‰æœ€æµè¡Œçš„CNNæ¨¡å‹ä¹‹ä¸€ï¼ŒVGGé€šè¿‡ä½¿ç”¨ä¸€ç³»åˆ—å¤§å°ä¸º3x3çš„å°å°ºå¯¸å·ç§¯æ ¸å’Œæ± åŒ–å±‚æ„é€ æ·±åº¦å·ç§¯ç¥ç»ç½‘ç»œï¼Œå¹¶å–å¾—äº†è¾ƒå¥½çš„æ•ˆæœã€‚VGGæ¨¡å‹å› ä¸ºç»“æ„ç®€å•ã€åº”ç”¨æ€§æå¼ºè€Œå¹¿å—ç ”ç©¶è€…æ¬¢è¿ï¼Œå°¤å…¶æ˜¯å®ƒçš„ç½‘ç»œç»“æ„è®¾è®¡æ–¹æ³•ï¼Œä¸ºæ„å»ºæ·±åº¦ç¥ç»ç½‘ç»œæä¾›äº†æ–¹å‘ã€‚

![1715001431647](image/02å›¾åƒåˆ†ç±»/1715001431647.png)

```
# -*- coding:utf-8 -*-

# VGGæ¨¡å‹ä»£ç 
import numpy as np
import paddle
# from paddle.nn import Conv2D, MaxPool2D, BatchNorm, Linear
from paddle.nn import Conv2D, MaxPool2D, BatchNorm2D, Linear

# å®šä¹‰vggç½‘ç»œ
class VGG(paddle.nn.Layer):
    def __init__(self):
        super(VGG, self).__init__()

        in_channels = [3, 64, 128, 256, 512, 512]
        # å®šä¹‰ç¬¬ä¸€ä¸ªblockï¼ŒåŒ…å«ä¸¤ä¸ªå·ç§¯
        self.conv1_1 = Conv2D(in_channels=in_channels[0], out_channels=in_channels[1], kernel_size=3, padding=1, stride=1)
        self.conv1_2 = Conv2D(in_channels=in_channels[1], out_channels=in_channels[1], kernel_size=3, padding=1, stride=1)
        # å®šä¹‰ç¬¬äºŒä¸ªblockï¼ŒåŒ…å«ä¸¤ä¸ªå·ç§¯
        self.conv2_1 = Conv2D(in_channels=in_channels[1], out_channels=in_channels[2], kernel_size=3, padding=1, stride=1)
        self.conv2_2 = Conv2D(in_channels=in_channels[2], out_channels=in_channels[2], kernel_size=3, padding=1, stride=1)
        # å®šä¹‰ç¬¬ä¸‰ä¸ªblockï¼ŒåŒ…å«ä¸‰ä¸ªå·ç§¯
        self.conv3_1 = Conv2D(in_channels=in_channels[2], out_channels=in_channels[3], kernel_size=3, padding=1, stride=1)
        self.conv3_2 = Conv2D(in_channels=in_channels[3], out_channels=in_channels[3], kernel_size=3, padding=1, stride=1)
        self.conv3_3 = Conv2D(in_channels=in_channels[3], out_channels=in_channels[3], kernel_size=3, padding=1, stride=1)
        # å®šä¹‰ç¬¬å››ä¸ªblockï¼ŒåŒ…å«ä¸‰ä¸ªå·ç§¯
        self.conv4_1 = Conv2D(in_channels=in_channels[3], out_channels=in_channels[4], kernel_size=3, padding=1, stride=1)
        self.conv4_2 = Conv2D(in_channels=in_channels[4], out_channels=in_channels[4], kernel_size=3, padding=1, stride=1)
        self.conv4_3 = Conv2D(in_channels=in_channels[4], out_channels=in_channels[4], kernel_size=3, padding=1, stride=1)
        # å®šä¹‰ç¬¬äº”ä¸ªblockï¼ŒåŒ…å«ä¸‰ä¸ªå·ç§¯
        self.conv5_1 = Conv2D(in_channels=in_channels[4], out_channels=in_channels[5], kernel_size=3, padding=1, stride=1)
        self.conv5_2 = Conv2D(in_channels=in_channels[5], out_channels=in_channels[5], kernel_size=3, padding=1, stride=1)
        self.conv5_3 = Conv2D(in_channels=in_channels[5], out_channels=in_channels[5], kernel_size=3, padding=1, stride=1)

        # ä½¿ç”¨Sequential å°†å…¨è¿æ¥å±‚å’Œreluç»„æˆä¸€ä¸ªçº¿æ€§ç»“æ„ï¼ˆfc + reluï¼‰
        # å½“è¾“å…¥ä¸º224x224æ—¶ï¼Œç»è¿‡äº”ä¸ªå·ç§¯å—å’Œæ± åŒ–å±‚åï¼Œç‰¹å¾ç»´åº¦å˜ä¸º[512x7x7]
        self.fc1 = paddle.nn.Sequential(paddle.nn.Linear(512 * 7 * 7, 4096), paddle.nn.ReLU())
        self.drop1_ratio = 0.5
        self.dropout1 = paddle.nn.Dropout(self.drop1_ratio, mode='upscale_in_train')
        # ä½¿ç”¨Sequential å°†å…¨è¿æ¥å±‚å’Œreluç»„æˆä¸€ä¸ªçº¿æ€§ç»“æ„ï¼ˆfc + reluï¼‰
        self.fc2 = paddle.nn.Sequential(paddle.nn.Linear(4096, 4096), paddle.nn.ReLU())

        self.drop2_ratio = 0.5
        self.dropout2 = paddle.nn.Dropout(self.drop2_ratio, mode='upscale_in_train')
        self.fc3 = paddle.nn.Linear(4096, 1)

        self.relu = paddle.nn.ReLU()
        self.pool = MaxPool2D(stride=2, kernel_size=2)

    def forward(self, x):
        x = self.relu(self.conv1_1(x))
        x = self.relu(self.conv1_2(x))
        x = self.pool(x)

        x = self.relu(self.conv2_1(x))
        x = self.relu(self.conv2_2(x))
        x = self.pool(x)

        x = self.relu(self.conv3_1(x))
        x = self.relu(self.conv3_2(x))
        x = self.relu(self.conv3_3(x))
        x = self.pool(x)

        x = self.relu(self.conv4_1(x))
        x = self.relu(self.conv4_2(x))
        x = self.relu(self.conv4_3(x))
        x = self.pool(x)

        x = self.relu(self.conv5_1(x))
        x = self.relu(self.conv5_2(x))
        x = self.relu(self.conv5_3(x))
        x = self.pool(x)

        x = paddle.flatten(x, 1, -1)
        x = self.dropout1(self.relu(self.fc1(x)))
        x = self.dropout2(self.relu(self.fc2(x)))
        x = self.fc3(x)
        return x
```

```
# åˆ›å»ºæ¨¡å‹
model = VGG()
# opt = paddle.optimizer.Adam(learning_rate=0.001, parameters=model.parameters())
opt = paddle.optimizer.Momentum(learning_rate=0.001, momentum=0.9, parameters=model.parameters())

# å¯åŠ¨è®­ç»ƒè¿‡ç¨‹
train_pm(model, opt)
```

é€šè¿‡è¿è¡Œç»“æœå¯ä»¥å‘ç°ï¼Œåœ¨çœ¼ç–¾ç­›æŸ¥æ•°æ®é›†iChallenge-PMä¸Šä½¿ç”¨VGGï¼Œlossèƒ½æœ‰æ•ˆçš„ä¸‹é™ï¼Œç»è¿‡5ä¸ªepochçš„è®­ç»ƒï¼Œåœ¨éªŒè¯é›†ä¸Šçš„å‡†ç¡®ç‡å¯ä»¥è¾¾åˆ°94%å·¦å³ã€‚


# GoogLeNet

å®ƒçš„ä¸»è¦ç‰¹ç‚¹æ˜¯ç½‘ç»œä¸ä»…æœ‰æ·±åº¦ï¼Œè¿˜åœ¨æ¨ªå‘ä¸Šå…·æœ‰â€œå®½åº¦â€ã€‚ç”±äºå›¾åƒä¿¡æ¯åœ¨ç©ºé—´å°ºå¯¸ä¸Šçš„å·¨å¤§å·®å¼‚ï¼Œå¦‚ä½•é€‰æ‹©åˆé€‚çš„å·ç§¯æ ¸æ¥æå–ç‰¹å¾å°±æ˜¾å¾—æ¯”è¾ƒå›°éš¾äº†ã€‚ç©ºé—´åˆ†å¸ƒèŒƒå›´æ›´å¹¿çš„å›¾åƒä¿¡æ¯é€‚åˆç”¨è¾ƒå¤§çš„å·ç§¯æ ¸æ¥æå–å…¶ç‰¹å¾ï¼›è€Œç©ºé—´åˆ†å¸ƒèŒƒå›´è¾ƒå°çš„å›¾åƒä¿¡æ¯åˆ™é€‚åˆç”¨è¾ƒå°çš„å·ç§¯æ ¸æ¥æå–å…¶ç‰¹å¾

![1715042320507](image/02å›¾åƒåˆ†ç±»/1715042320507.png)


![1715048519172](image/02å›¾åƒåˆ†ç±»/1715048519172.png)


# ResNet  æ®‹å·®ç½‘ç»œ


éšç€æ·±åº¦å­¦ä¹ çš„ä¸æ–­å‘å±•ï¼Œæ¨¡å‹çš„å±‚æ•°è¶Šæ¥è¶Šå¤šï¼Œç½‘ç»œç»“æ„ä¹Ÿè¶Šæ¥è¶Šå¤æ‚ã€‚é‚£ä¹ˆæ˜¯å¦åŠ æ·±ç½‘ç»œç»“æ„ï¼Œå°±ä¸€å®šä¼šå¾—åˆ°æ›´å¥½çš„æ•ˆæœå‘¢ï¼Ÿ

ä»ç†è®ºä¸Šæ¥è¯´ï¼Œå‡è®¾æ–°å¢åŠ çš„å±‚éƒ½æ˜¯æ’ç­‰æ˜ å°„ï¼Œåªè¦åŸæœ‰çš„å±‚å­¦å‡ºè·ŸåŸæ¨¡å‹ä¸€æ ·çš„å‚æ•°ï¼Œé‚£ä¹ˆæ·±æ¨¡å‹ç»“æ„å°±èƒ½è¾¾åˆ°åŸæ¨¡å‹ç»“æ„çš„æ•ˆæœã€‚æ¢å¥è¯è¯´ï¼ŒåŸæ¨¡å‹çš„è§£åªæ˜¯æ–°æ¨¡å‹çš„è§£çš„å­ç©ºé—´ï¼Œåœ¨æ–°æ¨¡å‹è§£çš„ç©ºé—´é‡Œåº”è¯¥èƒ½æ‰¾åˆ°æ¯”åŸæ¨¡å‹è§£å¯¹åº”çš„å­ç©ºé—´æ›´å¥½çš„ç»“æœã€‚ä½†æ˜¯å®è·µè¡¨æ˜ï¼Œå¢åŠ ç½‘ç»œçš„å±‚æ•°ä¹‹åï¼Œè®­ç»ƒè¯¯å·®å¾€å¾€ä¸é™åå‡ã€‚

æ®‹å·®ç½‘ç»œResNetæ¥è§£å†³ä¸Šè¿°é—®é¢˜

![1715063146498](image/02å›¾åƒåˆ†ç±»/1715063146498.png)

* å›¾6(a)ï¼šè¡¨ç¤ºå¢åŠ ç½‘ç»œçš„æ—¶å€™ï¼Œå°†ğ‘¥æ˜ å°„æˆğ‘¦=ğ¹(ğ‘¥)è¾“å‡ºã€‚
* å›¾6(b)ï¼šå¯¹å›¾6(a)ä½œäº†æ”¹è¿›ï¼Œè¾“å‡ºğ‘¦=ğ¹(ğ‘¥)+ğ‘¥ã€‚è¿™æ—¶ä¸æ˜¯ç›´æ¥å­¦ä¹ è¾“å‡ºç‰¹å¾ğ‘¦çš„è¡¨ç¤ºï¼Œè€Œæ˜¯å­¦ä¹ ğ‘¦âˆ’ã€‚
  * å¦‚æœæƒ³å­¦ä¹ å‡ºåŸæ¨¡å‹çš„è¡¨ç¤ºï¼Œåªéœ€å°†ğ¹(ğ‘¥)çš„å‚æ•°å…¨éƒ¨è®¾ç½®ä¸º0ï¼Œåˆ™ğ‘¦=ğ‘¥æ’ç­‰æ˜ å°„ã€‚
  * ğ¹(ğ‘¥)=ğ‘¦âˆ’ğ‘¥ä¹Ÿå«åšæ®‹å·®é¡¹ï¼Œå¦‚æœğ‘¥â†’ğ‘¦çš„æ˜ å°„æ¥è¿‘æ’ç­‰æ˜ å°„ï¼Œå›¾6(b)ä¸­é€šè¿‡å­¦ä¹ æ®‹å·®é¡¹ä¹Ÿæ¯”å›¾6(a)å­¦ä¹ å®Œæ•´æ˜ å°„å½¢å¼æ›´åŠ å®¹æ˜“ã€‚

ResNetæ¯å±‚éƒ½å­˜åœ¨ç›´è¿çš„æ—è·¯ï¼Œç›¸å½“äºæ¯ä¸€å±‚éƒ½å’Œæœ€ç»ˆçš„æŸå¤±æœ‰â€œç›´æ¥å¯¹è¯â€çš„æœºä¼šï¼Œè‡ªç„¶å¯ä»¥æ›´å¥½çš„è§£å†³æ¢¯åº¦å¼¥æ•£çš„é—®é¢˜ã€‚


ResNet-50çš„ç»“æ„ï¼Œä¸€å…±åŒ…å«49å±‚å·ç§¯å’Œ1å±‚å…¨è¿æ¥ï¼Œæ‰€ä»¥è¢«ç§°ä¸ºResNet-50

![1715063774329](image/02å›¾åƒåˆ†ç±»/1715063774329.png)


```
# -*- coding:utf-8 -*-

# ResNetæ¨¡å‹ä»£ç 
import numpy as np
import paddle
import paddle.nn as nn
import paddle.nn.functional as F

# ResNetä¸­ä½¿ç”¨äº†BatchNormå±‚ï¼Œåœ¨å·ç§¯å±‚çš„åé¢åŠ ä¸ŠBatchNormä»¥æå‡æ•°å€¼ç¨³å®šæ€§
# å®šä¹‰å·ç§¯æ‰¹å½’ä¸€åŒ–å—
class ConvBNLayer(paddle.nn.Layer):
    def __init__(self,
                 num_channels,
                 num_filters,
                 filter_size,
                 stride=1,
                 groups=1,
                 act=None):
     
        """
        num_channels, å·ç§¯å±‚çš„è¾“å…¥é€šé“æ•°
        num_filters, å·ç§¯å±‚çš„è¾“å‡ºé€šé“æ•°
        stride, å·ç§¯å±‚çš„æ­¥å¹…
        groups, åˆ†ç»„å·ç§¯çš„ç»„æ•°ï¼Œé»˜è®¤groups=1ä¸ä½¿ç”¨åˆ†ç»„å·ç§¯
        """
        super(ConvBNLayer, self).__init__()

        # åˆ›å»ºå·ç§¯å±‚
        self._conv = nn.Conv2D(
            in_channels=num_channels,
            out_channels=num_filters,
            kernel_size=filter_size,
            stride=stride,
            padding=(filter_size - 1) // 2,
            groups=groups,
            bias_attr=False)

        # åˆ›å»ºBatchNormå±‚
        self._batch_norm = paddle.nn.BatchNorm2D(num_filters)
      
        self.act = act

    def forward(self, inputs):
        y = self._conv(inputs)
        y = self._batch_norm(y)
        if self.act == 'leaky':
            y = F.leaky_relu(x=y, negative_slope=0.1)
        elif self.act == 'relu':
            y = F.relu(x=y)
        return y

# å®šä¹‰æ®‹å·®å—
# æ¯ä¸ªæ®‹å·®å—ä¼šå¯¹è¾“å…¥å›¾ç‰‡åšä¸‰æ¬¡å·ç§¯ï¼Œç„¶åè·Ÿè¾“å…¥å›¾ç‰‡è¿›è¡ŒçŸ­æ¥
# å¦‚æœæ®‹å·®å—ä¸­ç¬¬ä¸‰æ¬¡å·ç§¯è¾“å‡ºç‰¹å¾å›¾çš„å½¢çŠ¶ä¸è¾“å…¥ä¸ä¸€è‡´ï¼Œåˆ™å¯¹è¾“å…¥å›¾ç‰‡åš1x1å·ç§¯ï¼Œå°†å…¶è¾“å‡ºå½¢çŠ¶è°ƒæ•´æˆä¸€è‡´
class BottleneckBlock(paddle.nn.Layer):
    def __init__(self,
                 num_channels,
                 num_filters,
                 stride,
                 shortcut=True):
        super(BottleneckBlock, self).__init__()
        # åˆ›å»ºç¬¬ä¸€ä¸ªå·ç§¯å±‚ 1x1
        self.conv0 = ConvBNLayer(
            num_channels=num_channels,
            num_filters=num_filters,
            filter_size=1,
            act='relu')
        # åˆ›å»ºç¬¬äºŒä¸ªå·ç§¯å±‚ 3x3
        self.conv1 = ConvBNLayer(
            num_channels=num_filters,
            num_filters=num_filters,
            filter_size=3,
            stride=stride,
            act='relu')
        # åˆ›å»ºç¬¬ä¸‰ä¸ªå·ç§¯ 1x1ï¼Œä½†è¾“å‡ºé€šé“æ•°ä¹˜ä»¥4
        self.conv2 = ConvBNLayer(
            num_channels=num_filters,
            num_filters=num_filters * 4,
            filter_size=1,
            act=None)

        # å¦‚æœconv2çš„è¾“å‡ºè·Ÿæ­¤æ®‹å·®å—çš„è¾“å…¥æ•°æ®å½¢çŠ¶ä¸€è‡´ï¼Œåˆ™shortcut=True
        # å¦åˆ™shortcut = Falseï¼Œæ·»åŠ 1ä¸ª1x1çš„å·ç§¯ä½œç”¨åœ¨è¾“å…¥æ•°æ®ä¸Šï¼Œä½¿å…¶å½¢çŠ¶å˜æˆè·Ÿconv2ä¸€è‡´
        if not shortcut:
            self.short = ConvBNLayer(
                num_channels=num_channels,
                num_filters=num_filters * 4,
                filter_size=1,
                stride=stride)

        self.shortcut = shortcut

        self._num_channels_out = num_filters * 4

    def forward(self, inputs):
        y = self.conv0(inputs)
        conv1 = self.conv1(y)
        conv2 = self.conv2(conv1)

        # å¦‚æœshortcut=Trueï¼Œç›´æ¥å°†inputsè·Ÿconv2çš„è¾“å‡ºç›¸åŠ 
        # å¦åˆ™éœ€è¦å¯¹inputsè¿›è¡Œä¸€æ¬¡å·ç§¯ï¼Œå°†å½¢çŠ¶è°ƒæ•´æˆè·Ÿconv2è¾“å‡ºä¸€è‡´
        if self.shortcut:
            short = inputs
        else:
            short = self.short(inputs)

        y = paddle.add(x=short, y=conv2)
        y = F.relu(y)
        return y

# å®šä¹‰ResNetæ¨¡å‹
class ResNet(paddle.nn.Layer):
    def __init__(self, layers=50, class_dim=1):
        """
      
        layers, ç½‘ç»œå±‚æ•°ï¼Œå¯ä»¥æ˜¯50, 101æˆ–è€…152
        class_dimï¼Œåˆ†ç±»æ ‡ç­¾çš„ç±»åˆ«æ•°
        """
        super(ResNet, self).__init__()
        self.layers = layers
        supported_layers = [50, 101, 152]
        assert layers in supported_layers, \
            "supported layers are {} but input layer is {}".format(supported_layers, layers)

        if layers == 50:
            #ResNet50åŒ…å«å¤šä¸ªæ¨¡å—ï¼Œå…¶ä¸­ç¬¬2åˆ°ç¬¬5ä¸ªæ¨¡å—åˆ†åˆ«åŒ…å«3ã€4ã€6ã€3ä¸ªæ®‹å·®å—
            depth = [3, 4, 6, 3]
        elif layers == 101:
            #ResNet101åŒ…å«å¤šä¸ªæ¨¡å—ï¼Œå…¶ä¸­ç¬¬2åˆ°ç¬¬5ä¸ªæ¨¡å—åˆ†åˆ«åŒ…å«3ã€4ã€23ã€3ä¸ªæ®‹å·®å—
            depth = [3, 4, 23, 3]
        elif layers == 152:
            #ResNet152åŒ…å«å¤šä¸ªæ¨¡å—ï¼Œå…¶ä¸­ç¬¬2åˆ°ç¬¬5ä¸ªæ¨¡å—åˆ†åˆ«åŒ…å«3ã€8ã€36ã€3ä¸ªæ®‹å·®å—
            depth = [3, 8, 36, 3]
      
        # æ®‹å·®å—ä¸­ä½¿ç”¨åˆ°çš„å·ç§¯çš„è¾“å‡ºé€šé“æ•°
        num_filters = [64, 128, 256, 512]

        # ResNetçš„ç¬¬ä¸€ä¸ªæ¨¡å—ï¼ŒåŒ…å«1ä¸ª7x7å·ç§¯ï¼Œåé¢è·Ÿç€1ä¸ªæœ€å¤§æ± åŒ–å±‚
        self.conv = ConvBNLayer(
            num_channels=3,
            num_filters=64,
            filter_size=7,
            stride=2,
            act='relu')
        self.pool2d_max = nn.MaxPool2D(
            kernel_size=3,
            stride=2,
            padding=1)

        # ResNetçš„ç¬¬äºŒåˆ°ç¬¬äº”ä¸ªæ¨¡å—c2ã€c3ã€c4ã€c5
        self.bottleneck_block_list = []
        num_channels = 64
        for block in range(len(depth)):
            shortcut = False
            for i in range(depth[block]):
                # c3ã€c4ã€c5å°†ä¼šåœ¨ç¬¬ä¸€ä¸ªæ®‹å·®å—ä½¿ç”¨stride=2ï¼›å…¶ä½™æ‰€æœ‰æ®‹å·®å—stride=1
                bottleneck_block = self.add_sublayer(
                    'bb_%d_%d' % (block, i),
                    BottleneckBlock(
                        num_channels=num_channels,
                        num_filters=num_filters[block],
                        stride=2 if i == 0 and block != 0 else 1, 
                        shortcut=shortcut))
                num_channels = bottleneck_block._num_channels_out
                self.bottleneck_block_list.append(bottleneck_block)
                shortcut = True

        # åœ¨c5çš„è¾“å‡ºç‰¹å¾å›¾ä¸Šä½¿ç”¨å…¨å±€æ± åŒ–
        self.pool2d_avg = paddle.nn.AdaptiveAvgPool2D(output_size=1)

        # stdvç”¨æ¥ä½œä¸ºå…¨è¿æ¥å±‚éšæœºåˆå§‹åŒ–å‚æ•°çš„æ–¹å·®
        import math
        stdv = 1.0 / math.sqrt(2048 * 1.0)
      
        # åˆ›å»ºå…¨è¿æ¥å±‚ï¼Œè¾“å‡ºå¤§å°ä¸ºç±»åˆ«æ•°ç›®ï¼Œç»è¿‡æ®‹å·®ç½‘ç»œçš„å·ç§¯å’Œå…¨å±€æ± åŒ–åï¼Œ
        # å·ç§¯ç‰¹å¾çš„ç»´åº¦æ˜¯[B,2048,1,1]ï¼Œæ•…æœ€åä¸€å±‚å…¨è¿æ¥çš„è¾“å…¥ç»´åº¦æ˜¯2048
        self.out = nn.Linear(in_features=2048, out_features=class_dim,
                      weight_attr=paddle.ParamAttr(
                          initializer=paddle.nn.initializer.Uniform(-stdv, stdv)))

    def forward(self, inputs):
        y = self.conv(inputs)
        y = self.pool2d_max(y)
        for bottleneck_block in self.bottleneck_block_list:
            y = bottleneck_block(y)
        y = self.pool2d_avg(y)
        y = paddle.reshape(y, [y.shape[0], -1])
        y = self.out(y)
        return y
```

```
# åˆ›å»ºæ¨¡å‹
model = ResNet()
# å®šä¹‰ä¼˜åŒ–å™¨
opt = paddle.optimizer.Momentum(learning_rate=0.001, momentum=0.9, parameters=model.parameters(), weight_decay=0.001)
# å¯åŠ¨è®­ç»ƒè¿‡ç¨‹
train_pm(model, opt)
```



# ä½¿ç”¨é£æ¡¨é«˜å±‚APIç›´æ¥è°ƒç”¨å›¾åƒåˆ†ç±»ç½‘ç»œ

é«˜å±‚APIæ”¯æŒpaddle.vision.modelsæ¥å£ï¼Œå®ç°äº†å¯¹å¸¸ç”¨æ¨¡å‹çš„å°è£…ï¼ŒåŒ…æ‹¬ResNetã€VGGã€MobileNetã€LeNetç­‰ã€‚ä½¿ç”¨é«˜å±‚APIè°ƒç”¨è¿™äº›ç½‘ç»œï¼Œå¯ä»¥å¿«é€Ÿå®Œæˆç¥ç»ç½‘ç»œçš„è®­ç»ƒå’ŒFine-tuneã€‚


```
import paddle
from paddle.vision.models import resnet50

# è°ƒç”¨é«˜å±‚APIçš„resnet50æ¨¡å‹
model = resnet50()
# è®¾ç½®pretrainedå‚æ•°ä¸ºTrueï¼Œå¯ä»¥åŠ è½½resnet50åœ¨imagenetæ•°æ®é›†ä¸Šçš„é¢„è®­ç»ƒæ¨¡å‹
# model = resnet50(pretrained=True)

# éšæœºç”Ÿæˆä¸€ä¸ªè¾“å…¥
x = paddle.rand([1, 3, 224, 224])
# å¾—åˆ°æ®‹å·®50çš„è®¡ç®—ç»“æœ
out = model(x)
# æ‰“å°è¾“å‡ºçš„å½¢çŠ¶ï¼Œç”±äºresnet50é»˜è®¤çš„æ˜¯1000åˆ†ç±»
# æ‰€ä»¥è¾“å‡ºshapeæ˜¯[1x1000]
print(out.shape)
```

ä½¿ç”¨paddle.visionä¸­çš„æ¨¡å‹å¯ä»¥ç®€å•å¿«é€Ÿçš„æ„å»ºä¸€ä¸ªæ·±åº¦å­¦ä¹ ä»»åŠ¡ï¼Œå¦‚ä¸‹ç¤ºä¾‹ï¼Œä»…14è¡Œä»£ç å³å¯å®ç°resnetåœ¨Cifar10æ•°æ®é›†ä¸Šçš„è®­ç»ƒã€‚


```
# ä»paddle.vision.models æ¨¡å—ä¸­import æ®‹å·®ç½‘ç»œï¼ŒVGGç½‘ç»œï¼ŒLeNetç½‘ç»œ
from paddle.vision.models import resnet50, vgg16, LeNet
from paddle.vision.datasets import Cifar10
from paddle.optimizer import Momentum
from paddle.regularizer import L2Decay
from paddle.nn import CrossEntropyLoss
from paddle.metric import Accuracy
from paddle.vision.transforms import Transpose

# ç¡®ä¿ä»paddle.vision.datasets.Cifar10ä¸­åŠ è½½çš„å›¾åƒæ•°æ®æ˜¯np.ndarrayç±»å‹
paddle.vision.set_image_backend('cv2')
# è°ƒç”¨resnet50æ¨¡å‹
model = paddle.Model(resnet50(pretrained=False, num_classes=10))

# ä½¿ç”¨Cifar10æ•°æ®é›†
train_dataset = Cifar10(mode='train', transform=Transpose())
val_dataset = Cifar10(mode='test', transform=Transpose())
# å®šä¹‰ä¼˜åŒ–å™¨
optimizer = Momentum(learning_rate=0.01,
                     momentum=0.9,
                     weight_decay=L2Decay(1e-4),
                     parameters=model.parameters())
# è¿›è¡Œè®­ç»ƒå‰å‡†å¤‡
model.prepare(optimizer, CrossEntropyLoss(), Accuracy(topk=(1, 5)))
# å¯åŠ¨è®­ç»ƒ
model.fit(train_dataset,
          val_dataset,
          epochs=50,
          batch_size=64,
          save_dir="./output",
          num_workers=8)
```
